{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Category Classifier Model Training\n",
    "\n",
    "This notebook performs the model training for a text classifier using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Type: DistilBERT__\n",
    "\n",
    "This model was trained using a python v3.11.3 environment and requires:\n",
    "- spacy\n",
    "- spacy-transformers\n",
    "\n",
    "Please consult the requirements.txt for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.99\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\n"
     ]
    }
   ],
   "source": [
    "# check cuda and gpu status\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 23 11:35:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.120      Driver Version: 529.01       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   35C    P8     1W / 114W |     10MiB /  8188MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A        25      G   /Xwayland                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate config file and modify as necessary to use the correct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: textcat\n",
      "- Optimize for: efficiency\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config_transformer.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_transformer.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# generate the generic the config file\n",
    "!python -m spacy init config --pipeline textcat config_transformer.cfg --gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Edit the config file to use the correct transformer model\n",
    "\n",
    "\n",
    "1. in the section\n",
    "\n",
    "```\n",
    "[components.transformer.model]\n",
    "@architectures = \"spacy-transformers.TransformerModel.v3\"\n",
    "name = \"distilbert-base-uncased\"\n",
    "mixed_precision = false\n",
    "```\n",
    "\n",
    "2. save the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train the model and evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: textcat_model_transformer\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: textcat_model_transformer\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-06-23 11:43:45,670] [INFO] Set up nlp object from config\n",
      "[2023-06-23 11:43:45,676] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2023-06-23 11:43:45,678] [INFO] Created vocabulary\n",
      "[2023-06-23 11:43:45,678] [INFO] Finished initializing nlp object\n",
      "Downloading (…)okenizer_config.json: 100%|████| 28.0/28.0 [00:00<00:00, 140kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 483/483 [00:00<00:00, 4.91MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 232kB [00:00, 5.33MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 466kB [00:00, 8.19MB/s]\n",
      "Downloading model.safetensors: 100%|█████████| 268M/268M [00:23<00:00, 11.5MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2023-06-23 11:44:27,380] [INFO] Initialized pipeline components: ['transformer', 'textcat']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ------------  ----------  ------\n",
      "  0       0           0.00          0.07        0.68    0.01\n",
      "  0     200           1.61         14.32        5.01    0.05\n",
      "  0     400          18.48         12.44       19.88    0.20\n",
      "  0     600          42.99          9.99       25.56    0.26\n",
      "  0     800          62.77          8.73       33.93    0.34\n",
      "  0    1000          76.34          8.23       36.47    0.36\n",
      "  1    1200          80.24          7.09       39.63    0.40\n",
      "  1    1400          92.88          7.21       42.23    0.42\n",
      "  1    1600         101.41          6.86       44.57    0.45\n",
      "  1    1800         108.82          6.79       44.75    0.45\n",
      "  1    2000         114.95          6.78       46.53    0.47\n",
      "  2    2200         111.78          6.30       48.80    0.49\n",
      "  2    2400         105.88          5.38       48.85    0.49\n",
      "  2    2600         112.04          5.42       49.38    0.49\n",
      "  2    2800         121.13          5.39       49.61    0.50\n",
      "  2    3000         125.71          5.12       50.30    0.50\n",
      "  2    3200         127.98          5.15       50.33    0.50\n",
      "  3    3400         107.52          4.17       50.00    0.50\n",
      "  3    3600         110.34          4.09       50.36    0.50\n",
      "  3    3800         119.05          4.13       50.50    0.50\n",
      "  3    4000         121.18          4.01       50.04    0.50\n",
      "  3    4200         129.72          4.22       51.02    0.51\n",
      "  4    4400         102.62          3.50       50.61    0.51\n",
      "  4    4600         102.39          2.96       50.24    0.50\n",
      "  4    4800         106.40          2.94       50.86    0.51\n",
      "  4    5000         118.61          3.19       50.52    0.51\n",
      "  4    5200         109.64          3.13       50.63    0.51\n",
      "  5    5400         109.53          2.89       50.00    0.50\n",
      "  5    5600          85.38          2.40       50.85    0.51\n",
      "  5    5800          95.73          2.35       50.56    0.51\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "textcat_model_transformer/model-last\n"
     ]
    }
   ],
   "source": [
    "# train them model\n",
    "!python -m spacy train config_transformer.cfg --paths.train ../data/train.spacy  --paths.dev ../data/dev.spacy --output textcat_model_transformer --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK                 100.00\n",
      "TEXTCAT (macro F)   50.86 \n",
      "SPEED               5247  \n",
      "\n",
      "\u001b[1m\n",
      "=========================== Textcat F (per label) ===========================\u001b[0m\n",
      "\n",
      "                     P       R       F\n",
      "POLITICS         77.83   80.05   78.92\n",
      "WELLNESS         58.81   70.48   64.12\n",
      "ENTERTAINMENT    71.24   73.98   72.58\n",
      "TRAVEL           78.89   79.05   78.97\n",
      "HEALTHY LIVING   35.26   35.03   35.15\n",
      "BUSINESS         52.45   47.93   50.09\n",
      "WEIRD NEWS       40.36   47.08   43.46\n",
      "SPORTS           68.01   74.75   71.22\n",
      "PARENTING        51.73   67.42   58.54\n",
      "STYLE & BEAUTY   83.05   78.03   80.46\n",
      "GREEN            41.38   43.72   42.52\n",
      "FOOD & DRINK     70.45   74.96   72.63\n",
      "QUEER VOICES     71.81   68.08   69.90\n",
      "THE WORLDPOST    45.33   48.63   46.92\n",
      "HOME & LIVING    81.02   75.37   78.09\n",
      "WEDDINGS         77.75   77.55   77.65\n",
      "PARENTS          45.98   25.69   32.96\n",
      "ARTS & CULTURE   50.00   30.22   37.67\n",
      "CRIME            57.41   51.96   54.55\n",
      "CULTURE & ARTS   68.89   36.05   47.33\n",
      "ENVIRONMENT      56.25   20.30   29.83\n",
      "COMEDY           47.27   53.94   50.39\n",
      "RELIGION         59.34   55.43   57.31\n",
      "MONEY            47.33   43.03   45.08\n",
      "BLACK VOICES     50.12   48.50   49.30\n",
      "COLLEGE          41.07   41.82   41.44\n",
      "DIVORCE          81.61   69.71   75.19\n",
      "U.S. NEWS        22.22   12.28   15.82\n",
      "WORLD NEWS       43.67   28.82   34.72\n",
      "IMPACT           35.74   35.16   35.45\n",
      "STYLE            51.03   46.26   48.53\n",
      "EDUCATION        50.65   36.11   42.16\n",
      "WORLDPOST        36.36   36.80   36.58\n",
      "SCIENCE          63.13   55.56   59.10\n",
      "TASTE            40.17   25.82   31.44\n",
      "TECH             52.38   48.06   50.13\n",
      "WOMEN            33.15   35.59   34.33\n",
      "GOOD NEWS        33.57   32.88   33.22\n",
      "FIFTY            51.52   25.76   34.34\n",
      "ARTS             24.71   33.07   28.28\n",
      "MEDIA            56.48   59.65   58.02\n",
      "LATINO VOICES    57.29   47.41   51.89\n",
      "\n",
      "\u001b[1m\n",
      "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
      "\n",
      "                 ROC AUC\n",
      "POLITICS            0.96\n",
      "WELLNESS            0.96\n",
      "ENTERTAINMENT       0.96\n",
      "TRAVEL              0.98\n",
      "HEALTHY LIVING      0.93\n",
      "BUSINESS            0.94\n",
      "WEIRD NEWS          0.95\n",
      "SPORTS              0.98\n",
      "PARENTING           0.96\n",
      "STYLE & BEAUTY      0.98\n",
      "GREEN               0.96\n",
      "FOOD & DRINK        0.99\n",
      "QUEER VOICES        0.96\n",
      "THE WORLDPOST       0.97\n",
      "HOME & LIVING       0.98\n",
      "WEDDINGS            0.99\n",
      "PARENTS             0.96\n",
      "ARTS & CULTURE      0.95\n",
      "CRIME               0.97\n",
      "CULTURE & ARTS      0.95\n",
      "ENVIRONMENT         0.96\n",
      "COMEDY              0.92\n",
      "RELIGION            0.95\n",
      "MONEY               0.95\n",
      "BLACK VOICES        0.94\n",
      "COLLEGE             0.94\n",
      "DIVORCE             0.98\n",
      "U.S. NEWS           0.92\n",
      "WORLD NEWS          0.97\n",
      "IMPACT              0.91\n",
      "STYLE               0.96\n",
      "EDUCATION           0.94\n",
      "WORLDPOST           0.96\n",
      "SCIENCE             0.96\n",
      "TASTE               0.97\n",
      "TECH                0.95\n",
      "WOMEN               0.92\n",
      "GOOD NEWS           0.95\n",
      "FIFTY               0.91\n",
      "ARTS                0.92\n",
      "MEDIA               0.95\n",
      "LATINO VOICES       0.95\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to metrics_transformer.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "!python -m spacy evaluate ./textcat_model_transformer/model-best/ --output ./metrics_transformer.json ../data/test.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POLITICS': 0.00016067341493908316, 'WELLNESS': 0.0005015170318074524, 'ENTERTAINMENT': 9.730336751090363e-05, 'TRAVEL': 0.9956992864608765, 'HEALTHY LIVING': 0.0001028354890877381, 'BUSINESS': 0.0001245860185008496, 'WEIRD NEWS': 0.00013257964747026563, 'SPORTS': 8.15750245237723e-05, 'PARENTING': 0.00030307695851661265, 'STYLE & BEAUTY': 0.00045892540947534144, 'GREEN': 0.00010505902901059017, 'FOOD & DRINK': 0.00029052604804746807, 'QUEER VOICES': 4.714441456599161e-05, 'THE WORLDPOST': 4.2378153011668473e-05, 'HOME & LIVING': 0.00016575682093389332, 'WEDDINGS': 6.386010500136763e-05, 'PARENTS': 4.930210343445651e-05, 'ARTS & CULTURE': 1.0325259609089699e-05, 'CRIME': 3.628828562796116e-05, 'CULTURE & ARTS': 6.851810030639172e-05, 'ENVIRONMENT': 0.00017636224220041186, 'COMEDY': 5.965606396785006e-05, 'RELIGION': 1.7043454136000946e-05, 'MONEY': 7.017145253485069e-05, 'BLACK VOICES': 2.1419115000753663e-05, 'COLLEGE': 2.3839120331103913e-05, 'DIVORCE': 2.7171905458089896e-05, 'U.S. NEWS': 2.8337588446447626e-05, 'WORLD NEWS': 4.376509605208412e-05, 'IMPACT': 7.141319656511769e-05, 'STYLE': 7.986411947058514e-05, 'EDUCATION': 1.84187574632233e-05, 'WORLDPOST': 0.00022901876945979893, 'SCIENCE': 6.465514888986945e-05, 'TASTE': 5.896582661080174e-05, 'TECH': 3.749319148482755e-05, 'WOMEN': 8.112686373351607e-06, 'GOOD NEWS': 3.522988845361397e-05, 'FIFTY': 0.0001739784056553617, 'ARTS': 0.00018192639981862158, 'MEDIA': 1.7935655705514364e-05, 'LATINO VOICES': 1.373409031657502e-05}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "import spacy\n",
    "nlp = spacy.load(\"textcat_model_transformer/model-best\")\n",
    "doc=nlp(\"History is made: 10 new UK attractions for day trips and short breaks\")\n",
    "print(doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRAVEL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(doc.cats, key=doc.cats.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956992864608765"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.cats[\"TRAVEL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Package the model into a Zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip up the model-best\n",
    "\n",
    "import shutil\n",
    "\n",
    "model_best_path = \"textcat_model_transformer/model-best\"\n",
    "zipfile_name = \"textcat_model_transformer/model-best\"\n",
    "\n",
    "shutil.make_archive(zipfile_name, \"zip\", model_best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ To preserve models, please rename the folder. For example, \"textcat_model_transformer\" > \"textcat_model_transformer_2023-07-17_12-24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
