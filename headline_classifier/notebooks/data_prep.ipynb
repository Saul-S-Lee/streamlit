{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "Split the dataset into train, test, and dev and convert the raw json dataset into the Spacy DocBin format\n",
    "\n",
    "Adapted from \n",
    "https://catherinebreslin.medium.com/text-classification-with-spacy-3-0-d945e2e8fc44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lines(infile, outfile, row_indices):\n",
    "    \"\"\"Extract the lines from infile that are specified in row_indices\n",
    "\n",
    "    save the results to outfile\n",
    "    \"\"\"\n",
    "\n",
    "    with open(infile) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    outlines = []\n",
    "    for i in row_indices:\n",
    "        outlines.append(lines[i])\n",
    "\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.writelines(outlines)\n",
    "\n",
    "\n",
    "def convert(\n",
    "    infile,\n",
    "    outfile,\n",
    "    categories=['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL'],\n",
    "):\n",
    "    \"\"\" Convert the json array of documents from infile to the DocBin\n",
    "    \n",
    "    format. Also set the category flag.\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    db = DocBin()\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        l = json.loads(line)\n",
    "        if l[\"category\"] in categories:\n",
    "            doc = nlp.make_doc(l[\"headline\"])\n",
    "\n",
    "            # set default values for all category flags to 0\n",
    "            doc.cats = {category: 0 for category in categories}\n",
    "            doc.cats[l[\"category\"]] = 1\n",
    "            db.add(doc)\n",
    "    \n",
    "    print(f\"found {len(db)} rows\")\n",
    "    db.to_disk(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is an array of json blobs. Read in the json blobs into a list.\n",
    "\n",
    "Note: the notebook expects the raw data file to be stored in folder name `data` in the same path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/News_Category_Dataset_v3.json\", \"r\") as f1:\n",
    "    lines = f1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209527\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows\n",
    "rows = len(lines)\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique categories\n",
    "\n",
    "cat_list = []\n",
    "for line in lines:\n",
    "    line = json.loads(line)\n",
    "    cat_list.append(line[\"category\"])\n",
    "\n",
    "categories = set(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARTS',\n",
       " 'ARTS & CULTURE',\n",
       " 'BLACK VOICES',\n",
       " 'BUSINESS',\n",
       " 'COLLEGE',\n",
       " 'COMEDY',\n",
       " 'CRIME',\n",
       " 'CULTURE & ARTS',\n",
       " 'DIVORCE',\n",
       " 'EDUCATION',\n",
       " 'ENTERTAINMENT',\n",
       " 'ENVIRONMENT',\n",
       " 'FIFTY',\n",
       " 'FOOD & DRINK',\n",
       " 'GOOD NEWS',\n",
       " 'GREEN',\n",
       " 'HEALTHY LIVING',\n",
       " 'HOME & LIVING',\n",
       " 'IMPACT',\n",
       " 'LATINO VOICES',\n",
       " 'MEDIA',\n",
       " 'MONEY',\n",
       " 'PARENTING',\n",
       " 'PARENTS',\n",
       " 'POLITICS',\n",
       " 'QUEER VOICES',\n",
       " 'RELIGION',\n",
       " 'SCIENCE',\n",
       " 'SPORTS',\n",
       " 'STYLE',\n",
       " 'STYLE & BEAUTY',\n",
       " 'TASTE',\n",
       " 'TECH',\n",
       " 'THE WORLDPOST',\n",
       " 'TRAVEL',\n",
       " 'U.S. NEWS',\n",
       " 'WEDDINGS',\n",
       " 'WEIRD NEWS',\n",
       " 'WELLNESS',\n",
       " 'WOMEN',\n",
       " 'WORLD NEWS',\n",
       " 'WORLDPOST'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of categories\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split into train, dev, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the file into train, dev, and test datasets. The dev dataset is used as a validation set by Spacy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate a list of indices for all the rows. Shuffle the indices\n",
    "# to get a random sampling\n",
    "\n",
    "indices = [d for d in range(rows)]\n",
    "\n",
    "# set the random seed to make ensure repeatibility\n",
    "random.seed(42)\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices for the various sets\n",
    "# the dataset is quite large. only train on 100k rows to reduce computational time\n",
    "\n",
    "# train: 100k rows\n",
    "# dev: 20k rows\n",
    "# test: 20k\n",
    "\n",
    "test_indices = indices[0:20000]\n",
    "dev_indices = indices[20000:40000]\n",
    "train_indices = indices[40000:140000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filenames\n",
    "\n",
    "infile_name = \"data/News_Category_Dataset_v3.json\"\n",
    "test_json = \"data/test.json\"\n",
    "dev_json = \"data/dev.json\"\n",
    "train_json = \"data/train.json\"\n",
    "\n",
    "test_filename = \"data/test.spacy\"\n",
    "dev_filename = \"data/dev.spacy\"\n",
    "train_filename = \"data/train.spacy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call extract_lines to extract the lines from the main dataset\n",
    "\n",
    "extract_lines(infile_name, test_json, test_indices)\n",
    "extract_lines(infile_name, dev_json, dev_indices)\n",
    "extract_lines(infile_name, train_json, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 20000 rows\n",
      "found 20000 rows\n",
      "found 100000 rows\n"
     ]
    }
   ],
   "source": [
    "# convert the json format to the Spacy DocBin format\n",
    "convert(test_json, test_filename, categories)\n",
    "convert(dev_json, dev_filename, categories)\n",
    "convert(train_json, train_filename, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
