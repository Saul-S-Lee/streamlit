{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Category Classifier Model Training\n",
    "\n",
    "This notebook performs the model training for a text classifier using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Type: BERT__\n",
    "\n",
    "This model was trained using a python v3.11.3 environment and requires:\n",
    "- spacy\n",
    "- spacy-transformers\n",
    "\n",
    "Please consult the requirements.txt for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.99\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\n"
     ]
    }
   ],
   "source": [
    "# check cuda and gpu status\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 23 13:16:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.120      Driver Version: 529.01       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   34C    P8     1W / 115W |      0MiB /  8188MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A        25      G   /Xwayland                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate config file and modify as necessary to use the correct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: textcat\n",
      "- Optimize for: efficiency\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config_transformer.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_transformer.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# generate the generic the config file\n",
    "!python -m spacy init config --pipeline textcat config_transformer.cfg --gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: Edit the config file to use the correct transformer model__\n",
    "\n",
    "\n",
    "1. in the section\n",
    "\n",
    "```\n",
    "[components.transformer.model]\n",
    "@architectures = \"spacy-transformers.TransformerModel.v3\"\n",
    "name = \"bert-base-uncased\"\n",
    "mixed_precision = false\n",
    "```\n",
    "\n",
    "2. save the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train the model and evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: textcat_model_transformer\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: textcat_model_transformer\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-06-23 13:17:40,196] [INFO] Set up nlp object from config\n",
      "[2023-06-23 13:17:40,203] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2023-06-23 13:17:40,204] [INFO] Created vocabulary\n",
      "[2023-06-23 13:17:40,205] [INFO] Finished initializing nlp object\n",
      "Downloading (…)okenizer_config.json: 100%|████| 28.0/28.0 [00:00<00:00, 238kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 570/570 [00:00<00:00, 5.42MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 232kB [00:00, 7.66MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 466kB [00:00, 9.69MB/s]\n",
      "Downloading model.safetensors: 100%|█████████| 440M/440M [00:41<00:00, 10.6MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2023-06-23 13:18:40,283] [INFO] Initialized pipeline components: ['transformer', 'textcat']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ------------  ----------  ------\n",
      "  0       0           0.00          0.07        0.68    0.01\n",
      "  0     200           1.49         14.33        3.56    0.04\n",
      "  0     400          16.38         12.52       19.34    0.19\n",
      "  0     600          38.90         10.03       25.42    0.25\n",
      "  0     800          57.91          8.71       33.77    0.34\n",
      "  0    1000          70.48          8.12       38.09    0.38\n",
      "  1    1200          73.84          7.04       39.98    0.40\n",
      "  1    1400          85.51          7.09       42.92    0.43\n",
      "  1    1600          92.82          6.78       45.37    0.45\n",
      "  1    1800         100.71          6.63       46.02    0.46\n",
      "  1    2000         105.89          6.73       47.52    0.48\n",
      "  2    2200         102.91          6.14       48.96    0.49\n",
      "  2    2400          95.80          5.21       49.92    0.50\n",
      "  2    2600         101.76          5.09       50.05    0.50\n",
      "  2    2800         110.48          5.15       50.92    0.51\n",
      "  2    3000         115.83          4.91       50.88    0.51\n",
      "  2    3200         118.66          5.17       51.86    0.52\n",
      "  3    3400          97.85          3.87       51.36    0.51\n",
      "  3    3600          97.25          3.63       51.12    0.51\n",
      "  3    3800         106.67          3.72       51.37    0.51\n",
      "  3    4000         109.24          3.84       50.32    0.50\n",
      "  3    4200         116.14          3.91       52.20    0.52\n",
      "  4    4400          89.09          3.13       50.95    0.51\n",
      "  4    4600          86.72          2.63       50.96    0.51\n",
      "  4    4800          92.34          2.67       51.05    0.51\n",
      "  4    5000         100.09          2.79       51.06    0.51\n",
      "  4    5200          93.77          2.69       50.78    0.51\n",
      "  5    5400          93.18          2.52       50.97    0.51\n"
     ]
    }
   ],
   "source": [
    "# train them model\n",
    "!python -m spacy train config_transformer.cfg --paths.train ../data/train.spacy  --paths.dev ../data/dev.spacy --output textcat_model_transformer --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK                 100.00\n",
      "TEXTCAT (macro F)   51.67 \n",
      "SPEED               4386  \n",
      "\n",
      "\u001b[1m\n",
      "=========================== Textcat F (per label) ===========================\u001b[0m\n",
      "\n",
      "                     P       R       F\n",
      "POLITICS         78.44   80.73   79.57\n",
      "WELLNESS         59.16   66.86   62.78\n",
      "ENTERTAINMENT    72.26   70.91   71.58\n",
      "TRAVEL           79.03   80.53   79.77\n",
      "HEALTHY LIVING   34.06   35.86   34.94\n",
      "BUSINESS         50.26   47.26   48.72\n",
      "WEIRD NEWS       43.65   45.83   44.72\n",
      "SPORTS           71.18   74.34   72.73\n",
      "PARENTING        53.98   64.34   58.70\n",
      "STYLE & BEAUTY   80.46   78.92   79.68\n",
      "GREEN            42.86   43.72   43.29\n",
      "FOOD & DRINK     68.77   77.44   72.85\n",
      "QUEER VOICES     66.14   72.80   69.31\n",
      "THE WORLDPOST    48.18   44.38   46.20\n",
      "HOME & LIVING    80.31   78.11   79.19\n",
      "WEDDINGS         79.95   78.07   79.00\n",
      "PARENTS          42.17   26.18   32.31\n",
      "ARTS & CULTURE   39.42   29.50   33.74\n",
      "CRIME            60.52   52.23   56.07\n",
      "CULTURE & ARTS   81.08   34.88   48.78\n",
      "ENVIRONMENT      58.82   22.56   32.61\n",
      "COMEDY           53.33   51.45   52.38\n",
      "RELIGION         53.28   56.59   54.89\n",
      "MONEY            51.80   43.64   47.37\n",
      "BLACK VOICES     48.71   56.58   52.35\n",
      "COLLEGE          44.33   39.09   41.55\n",
      "DIVORCE          86.67   66.86   75.48\n",
      "U.S. NEWS        18.06   11.40   13.98\n",
      "WORLD NEWS       45.83   34.87   39.61\n",
      "IMPACT           31.50   35.16   33.23\n",
      "STYLE            44.49   49.07   46.67\n",
      "EDUCATION        44.34   43.52   43.93\n",
      "WORLDPOST        36.58   37.60   37.08\n",
      "SCIENCE          63.37   56.89   59.95\n",
      "TASTE            43.66   34.07   38.27\n",
      "TECH             56.82   48.54   52.36\n",
      "WOMEN            38.19   34.71   36.36\n",
      "GOOD NEWS        31.05   40.41   35.12\n",
      "FIFTY            53.85   31.82   40.00\n",
      "ARTS             24.87   37.80   30.00\n",
      "MEDIA            60.07   59.65   59.86\n",
      "LATINO VOICES    63.10   45.69   53.00\n",
      "\n",
      "\u001b[1m\n",
      "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
      "\n",
      "                 ROC AUC\n",
      "POLITICS            0.97\n",
      "WELLNESS            0.96\n",
      "ENTERTAINMENT       0.96\n",
      "TRAVEL              0.98\n",
      "HEALTHY LIVING      0.94\n",
      "BUSINESS            0.94\n",
      "WEIRD NEWS          0.95\n",
      "SPORTS              0.97\n",
      "PARENTING           0.96\n",
      "STYLE & BEAUTY      0.99\n",
      "GREEN               0.96\n",
      "FOOD & DRINK        0.99\n",
      "QUEER VOICES        0.97\n",
      "THE WORLDPOST       0.97\n",
      "HOME & LIVING       0.98\n",
      "WEDDINGS            0.99\n",
      "PARENTS             0.96\n",
      "ARTS & CULTURE      0.95\n",
      "CRIME               0.97\n",
      "CULTURE & ARTS      0.94\n",
      "ENVIRONMENT         0.95\n",
      "COMEDY              0.93\n",
      "RELIGION            0.96\n",
      "MONEY               0.94\n",
      "BLACK VOICES        0.94\n",
      "COLLEGE             0.93\n",
      "DIVORCE             0.97\n",
      "U.S. NEWS           0.93\n",
      "WORLD NEWS          0.96\n",
      "IMPACT              0.91\n",
      "STYLE               0.96\n",
      "EDUCATION           0.94\n",
      "WORLDPOST           0.95\n",
      "SCIENCE             0.96\n",
      "TASTE               0.98\n",
      "TECH                0.96\n",
      "WOMEN               0.92\n",
      "GOOD NEWS           0.95\n",
      "FIFTY               0.91\n",
      "ARTS                0.92\n",
      "MEDIA               0.96\n",
      "LATINO VOICES       0.94\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to metrics_transformer.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "!python -m spacy evaluate ./textcat_model_transformer/model-best/ --output ./metrics_transformer.json ../data/test.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POLITICS': 6.087727160775103e-05, 'WELLNESS': 0.00016755890101194382, 'ENTERTAINMENT': 0.00012017521657980978, 'TRAVEL': 0.9958972930908203, 'HEALTHY LIVING': 4.173409251961857e-05, 'BUSINESS': 0.00016323034651577473, 'WEIRD NEWS': 0.0003394785162527114, 'SPORTS': 6.078213482396677e-05, 'PARENTING': 8.394128235522658e-05, 'STYLE & BEAUTY': 0.0003491932584438473, 'GREEN': 0.00011509830073919147, 'FOOD & DRINK': 0.0004563691036310047, 'QUEER VOICES': 4.171185355517082e-05, 'THE WORLDPOST': 4.907382026431151e-05, 'HOME & LIVING': 0.0001826801453717053, 'WEDDINGS': 7.545178959844634e-05, 'PARENTS': 2.223265983047895e-05, 'ARTS & CULTURE': 1.4923170965630561e-05, 'CRIME': 5.273676651995629e-05, 'CULTURE & ARTS': 9.494981350144371e-05, 'ENVIRONMENT': 0.0003040886949747801, 'COMEDY': 4.834379069507122e-05, 'RELIGION': 1.7640128135099076e-05, 'MONEY': 8.6573651060462e-05, 'BLACK VOICES': 2.124297861882951e-05, 'COLLEGE': 1.541004348837305e-05, 'DIVORCE': 2.8447317163227126e-05, 'U.S. NEWS': 5.008530570194125e-05, 'WORLD NEWS': 5.854940172866918e-05, 'IMPACT': 2.8829483198933303e-05, 'STYLE': 9.99500261968933e-05, 'EDUCATION': 1.083144798030844e-05, 'WORLDPOST': 0.00021056599507573992, 'SCIENCE': 5.873530244571157e-05, 'TASTE': 0.00012038214481435716, 'TECH': 6.094372656662017e-05, 'WOMEN': 3.839721557596931e-06, 'GOOD NEWS': 3.0093988243606873e-05, 'FIFTY': 0.00012690515723079443, 'ARTS': 0.0001903283118735999, 'MEDIA': 1.8325345081393607e-05, 'LATINO VOICES': 2.0378560293465853e-05}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "import spacy\n",
    "nlp = spacy.load(\"textcat_model_transformer/model-best\")\n",
    "doc=nlp(\"History is made: 10 new UK attractions for day trips and short breaks\")\n",
    "print(doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRAVEL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(doc.cats, key=doc.cats.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958972930908203"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.cats[\"TRAVEL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Package the model into a Zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip up the model-best\n",
    "\n",
    "import shutil\n",
    "\n",
    "model_best_path = \"textcat_model_transformer/model-best\"\n",
    "zipfile_name = \"textcat_model_transformer/model-best\"\n",
    "\n",
    "shutil.make_archive(zipfile_name, \"zip\", model_best_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ To preserve models, please rename the folder. For example, \"textcat_model_transformer\" > \"textcat_model_transformer_2023-07-17_12-24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
